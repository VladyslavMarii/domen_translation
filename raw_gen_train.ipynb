{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 09:05:04.329884: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750842304.359502    1141 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750842304.368281    1141 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-25 09:05:04.398078: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1750842319.487382    1141 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2245 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Notebook Setup & Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "parent_dir = current_dir.parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from raw_gen_model.raw_gen_encoders.raw_gen_drone_encoder import DroneEncoder\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "from gen_dataset_generator import create_datasets\n",
    "from helper import  set_up_environment\n",
    "from config import EPOCHS, TRAIN_DATASET_FILE_PATH, VAL_DATASET_FILE_PATH\n",
    "from tensorflow.keras import backend as K\n",
    "import tf_keras\n",
    "from tf_keras.saving import register_keras_serializable\n",
    "from tqdm import tqdm\n",
    " \n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define the loss\n",
    "@register_keras_serializable()\n",
    "class TranslationLoss(tf_keras.losses.Loss):\n",
    "    def __init__(self, name=\"translation_loss\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        sat_like_tokens = y_pred\n",
    "        real_sat_tokens = y_true\n",
    "\n",
    "        # We can do a simple L2 (MSE) or L1 or even a cosine similarity loss\n",
    "        # For example, L2:\n",
    "        diff = sat_like_tokens - real_sat_tokens\n",
    "        mse  = tf.reduce_mean(tf.square(diff))\n",
    "\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Step & Loop\n",
    "\n",
    "\"\"\"\n",
    "We can re-use the general train_step logic, but adapt it to the new two-branch approach.\n",
    "\"\"\"\n",
    "\n",
    "@tf.function\n",
    "def train_step(drone_encoder, satellite_encoder, optimizer, loss_fn, drone_batch, sat_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "            drone_tokens = satellite_encoder(drone_batch)\n",
    "            # Produce satellite-like tokens from drone images.\n",
    "            predicted_tokens, _ = drone_encoder(drone_tokens, training=True)\n",
    "            # Compute the loss between the predicted tokens and the fixed satellite tokens.\n",
    "            satellite_tokens = satellite_encoder(sat_batch)\n",
    "            loss = loss_fn(satellite_tokens, predicted_tokens)\n",
    "\n",
    "    # Update only the DroneEncoder's parameters.\n",
    "    gradients = tape.gradient(loss, drone_encoder.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, drone_encoder.trainable_variables))\n",
    "\n",
    "    return loss, predicted_tokens\n",
    "\n",
    "def train_model(drone_encoder: DroneEncoder, satellite_encoder, train_dataset, val_dataset, epochs, optimizer, callbacks=None):\n",
    "    train_loss_metric = tf_keras.metrics.Mean()\n",
    "    val_loss_metric   = tf_keras.metrics.Mean()\n",
    "\n",
    "    loss_fn = TranslationLoss()\n",
    "\n",
    "    # Initialize callbacks if provided\n",
    "    callbacks = callbacks or []\n",
    "    for callback in callbacks:\n",
    "        callback.drone_model = drone_encoder\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        for callback in callbacks:\n",
    "            callback.on_epoch_begin(epoch)\n",
    "\n",
    "        # --- TRAIN LOOP ---\n",
    "        train_loss_metric.reset_states()\n",
    "        with tqdm(total=len(train_dataset), desc=\"Training\", ncols=100, leave=False) as pbar:\n",
    "            for step, (sat_batch, drone_batch) in enumerate(train_dataset):\n",
    "                for callback in callbacks:\n",
    "                    callback.on_batch_begin(step)\n",
    "\n",
    "                loss, outputs = train_step(drone_encoder, satellite_encoder, optimizer, loss_fn, drone_batch, sat_batch)\n",
    "                train_loss_metric.update_state(loss)\n",
    "\n",
    "                pbar.set_postfix({\"loss\": f\"{train_loss_metric.result().numpy():.4f}\"})\n",
    "                pbar.update(1)\n",
    "\n",
    "                for callback in callbacks:\n",
    "                    callback.on_batch_end(step, {'loss': loss.numpy()})\n",
    "        \n",
    "        # --- VALIDATION LOOP ---\n",
    "        val_loss_metric.reset_states()\n",
    "        for step, (drone_batch, sat_batch) in enumerate(val_dataset):\n",
    "            drone_tokens = satellite_encoder(drone_batch)\n",
    "            sat_like_tokens, _ = drone_encoder(drone_tokens, training=False)\n",
    "            real_sat_tokens = satellite_encoder(sat_batch)\n",
    "            loss = loss_fn(real_sat_tokens,sat_like_tokens)\n",
    "            val_loss_metric.update_state(loss)\n",
    "\n",
    "        train_loss_val = train_loss_metric.result().numpy()\n",
    "        val_loss_val   = val_loss_metric.result().numpy()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: train_loss={train_loss_val:.4f} - val_loss={val_loss_val:.4f}\")\n",
    "\n",
    "        # on_epoch_end\n",
    "        logs = {'train_loss': train_loss_val, 'val_loss': val_loss_val}\n",
    "        for callback in callbacks:\n",
    "            callback.on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'raw_gen_model.raw_gen_callbacks.raw_gen_token_viz_callback'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mraw_gen_model\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mraw_gen_callbacks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mraw_get_save_model_callback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SaveBestRawModelCallback\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IMAGE_CHANNELS, IMAGE_RESOLUTION, LOGS_DIR, TRAINING_ID\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mraw_gen_model\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mraw_gen_callbacks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mraw_gen_token_viz_callback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TokenVisualizationCallback\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mraw_gen_model\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mraw_gen_callbacks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mraw_gen_attention_callback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AttentionVisualizationCallback\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mraw_gen_model\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mraw_gen_callbacks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mraw_gen_token_match_callback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TokenMatchingCallback\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'raw_gen_model.raw_gen_callbacks.raw_gen_token_viz_callback'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "# Model Initialization & Run\n",
    "from raw_gen_model.raw_gen_config import PATCH_SIZE\n",
    "from raw_gen_model.raw_gen_helper import create_satellite_decoder, create_satellite_encoder\n",
    "from raw_gen_model.raw_gen_callbacks.raw_get_save_model_callback import SaveBestRawModelCallback\n",
    "from config import IMAGE_CHANNELS, IMAGE_RESOLUTION, LOGS_DIR, TRAINING_ID\n",
    "from raw_gen_model.raw_gen_callbacks.raw_gen_attention_callback import AttentionVisualizationCallback\n",
    "from raw_gen_model.raw_gen_callbacks.raw_gen_token_match_callback import TokenMatchingCallback\n",
    "from raw_gen_model.raw_gen_callbacks.raw_gen_image_transform_callback import ImageTransformCallback\n",
    "\n",
    "set_up_environment()\n",
    "\n",
    "# Environment & Data Loading\n",
    "# Create the train & validation datasets\n",
    "train_dataset, val_dataset = create_datasets(TRAIN_DATASET_FILE_PATH, VAL_DATASET_FILE_PATH)\n",
    "\n",
    "train_steps = len(train_dataset)\n",
    "val_steps   = len(val_dataset)\n",
    "\n",
    "print(\"Number of training steps:\", train_steps)\n",
    "print(\"Number of validation steps:\", val_steps)\n",
    "\n",
    "\n",
    "drone_to_sat = DroneEncoder()\n",
    "\n",
    "satellite_encoder = create_satellite_encoder(patch_size=PATCH_SIZE, input_shape=(IMAGE_RESOLUTION[\"height\"], IMAGE_RESOLUTION[\"width\"], IMAGE_CHANNELS))\n",
    "satellite_decoder = create_satellite_decoder(patch_size=PATCH_SIZE, input_shape=(IMAGE_RESOLUTION[\"height\"], IMAGE_RESOLUTION[\"width\"], IMAGE_CHANNELS))\n",
    "\n",
    "# Adam or any other optimizer \n",
    "optimizer = tf_keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "\n",
    "attention_callback = AttentionVisualizationCallback(\n",
    "        validation_data=val_dataset,\n",
    "        log_dir= LOGS_DIR / \"attention_visualizations\",\n",
    "        save_every_n_epochs=1\n",
    "    )\n",
    "attention_callback.set_model(drone_to_sat)\n",
    "attention_callback.set_encoder(satellite_encoder)\n",
    "\n",
    "token_matching_callback = TokenMatchingCallback(validation_data=val_dataset)\n",
    "token_matching_callback.set_model(drone_to_sat)\n",
    "token_matching_callback.set_encoder(satellite_encoder)\n",
    "\n",
    "image_transform_callback = ImageTransformCallback(\n",
    "    validation_data=val_dataset,\n",
    "    encoder=satellite_encoder,\n",
    "    decoder=satellite_decoder,\n",
    "    save_dir=LOGS_DIR /\"image_transformations\",\n",
    ")\n",
    "image_transform_callback.set_model(drone_to_sat)\n",
    "save_model_callback = SaveBestRawModelCallback(drone_to_sat)\n",
    "\n",
    "# Add checkpoint callback to the callbacks list\n",
    "callbacks = [\n",
    "    attention_callback, \n",
    "    token_matching_callback, \n",
    "    image_transform_callback,\n",
    "    save_model_callback\n",
    "]\n",
    "# Train\n",
    "train_model(\n",
    "    drone_to_sat,\n",
    "    satellite_encoder,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=optimizer,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "print(\"TRAINING COMPLETE\")\n",
    "# After training completes...\n",
    "drone_encoder = None\n",
    "dataset = None\n",
    "K.clear_session()  # Clears backend, freeing session references\n",
    "gc.collect()       # Triggers Python garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
